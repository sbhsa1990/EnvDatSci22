{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy65tB7NAWRy"
      },
      "source": [
        "# Spatial Vector Data in Python w/ Geopandas\n",
        "You will use the geopandas library to work with vector data in Python. Geopandas is built on top of the Python Pandas library. It stores spatial data in a tabular, dataframe format.\n",
        "\n",
        "This notebook will take you through [Lesson 3 in Introductory Earth Data Science](https://www.earthdatascience.org/courses/intro-to-earth-data-science/file-formats/use-spatial-data/use-vector-data/) with some additional exercises from the [Geopandas project page](https://geopandas.org/en/v0.8.2/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd7IojYFAWR0"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LizCarter492/EnvDatSci22/blob/master/CodeSprints/VectorData101.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sync your colab environment with your Google Drive and GitHub"
      ],
      "metadata": {
        "id": "AoZASwHLwpAS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n63jAsimAWR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da7a0003-cd42-4721-a352-20bb06e7a6c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#attach to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s6oLyamZAWR2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "outputId": "11cded06-e50d-4ed6-f25a-f3a931ad8d82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting clmutils\n",
            "  Downloading clmutils-0.1.5-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: pydantic[dotenv]<2.0.0,>=1.7.3 in /usr/local/lib/python3.7/dist-packages (from clmutils) (1.9.2)\n",
            "Collecting psutil<6.0.0,>=5.8.0\n",
            "  Downloading psutil-5.9.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[K     |████████████████████████████████| 281 kB 25.9 MB/s \n",
            "\u001b[?25hCollecting logzero<2.0.0,>=1.6.3\n",
            "  Downloading logzero-1.7.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting chardet<5.0.0,>=4.0.0\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic[dotenv]<2.0.0,>=1.7.3->clmutils) (4.1.1)\n",
            "Collecting python-dotenv>=0.10.4\n",
            "  Downloading python_dotenv-0.21.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: python-dotenv, psutil, logzero, chardet, clmutils\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 3.0.4\n",
            "    Uninstalling chardet-3.0.4:\n",
            "      Successfully uninstalled chardet-3.0.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires chardet<4,>=3.0.2, but you have chardet 4.0.0 which is incompatible.\u001b[0m\n",
            "Successfully installed chardet-4.0.0 clmutils-0.1.5 logzero-1.7.0 psutil-5.9.2 python-dotenv-0.21.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 220929 18:12:40 chmod600:24] /root/.ssh/gh_sbhsa1990 mode set to 0o100600\n",
            "[I 220929 18:12:40 run_cmd1:33] ssh -T git@github.com -o StrictHostKeyChecking=no...\n",
            "    \n",
            "[E 220929 18:12:40 run_cmd1:35] Warning: Permanently added 'github.com,140.82.113.3' (ECDSA) to the list of known hosts.\n",
            "    Load key \"/root/.ssh/gh_sbhsa1990\": invalid format\n",
            "    git@github.com: Permission denied (publickey).\n",
            "    ,\n",
            "[W 220929 18:12:40 setup_git:161] \n",
            "     There appears to be some problem.\n",
            "    You may wish to exam the debug messages above, \n",
            "    fix it and give it another try.\n"
          ]
        }
      ],
      "source": [
        "#Connect to GitHub\n",
        "!pip install clmutils\n",
        "from clmutils import setup_git, Settings\n",
        "# note, for this to work, you must create a .env folder in your Google Drive with your username, email, and gc token. Example:\n",
        "# !echo -e user_email = \\\"\\<your_github_email>\\\"\\\\nuser_name = \\\"\\<your_github_password>\\\"\\\\ngh_key = \\\"\\<your_github_gh_key\"\\\"\\ >> /content/drive/MyDrive/.env\n",
        "\n",
        "config = Settings()\n",
        "setup_git(\n",
        "    user_name=config.user_name,\n",
        "    user_email=config.user_email,\n",
        "    priv_key=config.gh_key\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update your local (to Google Drive) EnvDatSci22 repo OR download your forked repository if you've never done so before\n",
        "# MAKE SURE TO EDIT THIS CODE TO PUT YOUR GITHUB USERNAME!!!\n",
        "\n",
        "!git -C EnvDatSci22 pull || git clone https://github.com/sbhsa1990/EnvDatSci22 EnvDatSci22"
      ],
      "metadata": {
        "id": "q1JXhrDbCDjr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f412a8af-d186-411d-9330-a80747fb52dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: cannot change to 'EnvDatSci22': No such file or directory\n",
            "Cloning into 'EnvDatSci22'...\n",
            "remote: Enumerating objects: 107, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 107 (delta 25), reused 23 (delta 23), pack-reused 78\u001b[K\n",
            "Receiving objects: 100% (107/107), 1.33 MiB | 25.64 MiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/drive/MyDrive/.env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOpZo6WNdSdK",
        "outputId": "8168252f-70c7-4a40-8347-8c1b1bf0058d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_email = \"sbhsa1990@gmail.com\"\n",
            "user_name = \"sbhsa1990\"\n",
            "gh_key = \"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\" \n",
            "user_email = \"sbhsa1990@gmail.com\"\n",
            "user_name = \"sbhsa1990\"\n",
            "gh_key = \"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\" \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd EnvDatSci\n",
        "\n",
        "# Hi I'm Babak"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXTjR7Kjf6Cc",
        "outputId": "dc33ae92-1288-4455-b832-c3f0bca2508c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 0: cd: EnvDatSci: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Code Sprint"
      ],
      "metadata": {
        "id": "WmOwykNxwvL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas"
      ],
      "metadata": {
        "id": "9KyeM4C89orB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6df100f-d8d3-48cf-84b7-1912eba46fa7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting geopandas\n",
            "  Downloading geopandas-0.10.2-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40 kB 27.3 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 61 kB 27.1 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 133 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 143 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████                           | 163 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 194 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 204 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 215 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 225 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████                        | 256 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 266 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 276 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 286 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 317 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 327 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 337 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 348 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 358 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 378 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 399 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 409 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 430 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 440 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 450 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 460 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 471 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 481 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 491 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 501 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 512 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 522 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 532 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 542 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 552 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 563 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 573 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 593 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 604 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 624 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 634 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 645 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 655 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 665 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 675 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 686 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 696 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 706 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 716 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 727 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 737 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 747 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 757 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 768 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 778 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 788 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 798 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 819 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 829 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 839 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 849 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 860 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 870 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 880 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 890 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 901 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 911 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 921 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 931 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 942 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 952 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 962 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 972 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 983 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 993 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0 MB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0 MB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.0 MB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0 MB 25.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: shapely>=1.6 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.8.4)\n",
            "Collecting fiona>=1.8\n",
            "  Downloading Fiona-1.8.21-cp37-cp37m-manylinux2014_x86_64.whl (16.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.7 MB 356 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.3.5)\n",
            "Collecting pyproj>=2.2.0\n",
            "  Downloading pyproj-3.2.1-cp37-cp37m-manylinux2010_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 47.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (2022.6.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (57.4.0)\n",
            "Collecting click-plugins>=1.0\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting cligj>=0.5\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (1.15.0)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (22.1.0)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (2022.2.1)\n",
            "Installing collected packages: munch, cligj, click-plugins, pyproj, fiona, geopandas\n",
            "Successfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.8.21 geopandas-0.10.2 munch-2.5.0 pyproj-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install earthpy\n",
        "!pip install rtree\n",
        "!pip install pygeos\n",
        "!pip install mapclassify>=2.4.0"
      ],
      "metadata": {
        "id": "xgMUKEkJLmB6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4649ea10-a8d0-41f8-93ea-e1ad1fc13dde"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting earthpy\n",
            "  Downloading earthpy-0.9.4-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 28.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from earthpy) (3.2.2)\n",
            "Collecting rasterio\n",
            "  Downloading rasterio-1.2.10-cp37-cp37m-manylinux1_x86_64.whl (19.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.3 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: geopandas in /usr/local/lib/python3.7/dist-packages (from earthpy) (0.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from earthpy) (2.23.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from earthpy) (0.18.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from earthpy) (1.21.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->earthpy) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->earthpy) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->earthpy) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->earthpy) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->earthpy) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.0.0->earthpy) (1.15.0)\n",
            "Requirement already satisfied: pyproj>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from geopandas->earthpy) (3.2.1)\n",
            "Requirement already satisfied: fiona>=1.8 in /usr/local/lib/python3.7/dist-packages (from geopandas->earthpy) (1.8.21)\n",
            "Requirement already satisfied: shapely>=1.6 in /usr/local/lib/python3.7/dist-packages (from geopandas->earthpy) (1.8.4)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from geopandas->earthpy) (1.3.5)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas->earthpy) (2022.6.15)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas->earthpy) (22.1.0)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas->earthpy) (1.1.1)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas->earthpy) (2.5.0)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas->earthpy) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas->earthpy) (57.4.0)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas->earthpy) (0.7.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas->earthpy) (2022.2.1)\n",
            "Collecting snuggs>=1.4.1\n",
            "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
            "Collecting affine\n",
            "  Downloading affine-2.3.1-py2.py3-none-any.whl (16 kB)\n",
            "Collecting chardet<4,>=3.0.2\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 59.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->earthpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->earthpy) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->earthpy) (1.7.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->earthpy) (2.9.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->earthpy) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->earthpy) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->earthpy) (2.6.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->earthpy) (7.1.2)\n",
            "Installing collected packages: snuggs, chardet, affine, rasterio, earthpy\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 4.0.0\n",
            "    Uninstalling chardet-4.0.0:\n",
            "      Successfully uninstalled chardet-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "clmutils 0.1.5 requires chardet<5.0.0,>=4.0.0, but you have chardet 3.0.4 which is incompatible.\u001b[0m\n",
            "Successfully installed affine-2.3.1 chardet-3.0.4 earthpy-0.9.4 rasterio-1.2.10 snuggs-1.4.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "chardet"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rtree\n",
            "  Downloading Rtree-1.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 22.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7 in /usr/local/lib/python3.7/dist-packages (from rtree) (4.1.1)\n",
            "Installing collected packages: rtree\n",
            "Successfully installed rtree-1.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pygeos\n",
            "  Downloading pygeos-0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 26.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.7/dist-packages (from pygeos) (1.21.6)\n",
            "Installing collected packages: pygeos\n",
            "Successfully installed pygeos-0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "import earthpy as et\n"
      ],
      "metadata": {
        "id": "fxgQBbU3Oxbl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sE97xoBAWR4"
      },
      "source": [
        "To begin, set your working directory to earth-analytics and then download a single shapefile. \n",
        "\n",
        "You will start with working with the Natural Earth country boundary lines layer: https://www.naturalearthdata.com/downloads/\n",
        "\n",
        "Note that below you are using EarthPy to download a dataset from naturalearthdata.com (via Amazon Web Services). \n",
        "\n",
        "EarthPy automatically creates the earth-analytics directory for you when you use it, but by default makes this directory in your home directory. We're doing this here because you guys are on PCs.\n",
        "\n",
        "You set the working directory after you download the data as a precaution to ensure that the earth-analytics directory already exists on your computer. This is not a standard order of operations, as we are not using our SU H drive, but we are demonstrating it here to ensure the notebook runs on all computers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HX73ilrbAWR5"
      },
      "outputs": [],
      "source": [
        "# Download a political boundaries shapefile and set your working directory\n",
        "et.data.get_data(\n",
        "    url='https://naturalearth.s3.amazonaws.com/50m_cultural/ne_50m_admin_0_boundary_lines_land.zip')\n",
        "\n",
        "# Set working directory - earthpy creates earth-analytics for you in your home dir\n",
        "os.chdir(os.path.join(et.io.HOME, 'earth-analytics'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1FhqaeJAWR5"
      },
      "source": [
        "### geopandas\n",
        "Next, you open the data using geopandas. \n",
        "\n",
        "***geopandas*** takes all of the data science magic from the pandas library and makes it compatible with shapfiles.\n",
        "\n",
        "Learn more here: https://geopandas.org/\n",
        "\n",
        "You can view the first 5 rows of the data using .head() in the same way you used .head() for Pandas dataframes.¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daU0NJvsAWR6"
      },
      "outputs": [],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNBjUKF1AWR7"
      },
      "source": [
        "We're going to download a second dataset, called coastlines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "im1CUtUbAWR8"
      },
      "outputs": [],
      "source": [
        "#set URL for the dataset\n",
        "coastlines_url = \"https://naturalearth.s3.amazonaws.com/50m_physical/ne_50m_coastline.zip\"\n",
        "\n",
        "#download the URL into your earth-analytics folder\n",
        "et.data.get_data(url=coastlines_url)\n",
        "\n",
        "#name the filepath relative to your working directory: note this is operating system agnostic\n",
        "coastlines_path = os.path.join(\"data\", \"earthpy-downloads\",\n",
        "                               \"ne_50m_coastline\",\n",
        "                               \"ne_50m_coastline.shp\")\n",
        "\n",
        "#use geopandas as gpd to \"read_file\"\n",
        "coastlines = gpd.read_file(coastlines_path)\n",
        "\n",
        "#prints \"head\" just like pandas!\n",
        "coastlines.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWVggDwWAWR9"
      },
      "source": [
        "## GeoPandas creates GeoDataFrames \n",
        "The structure of a Geopandas GeoDataFrame is very similar to a Pandas dataframe. This means that all the awesome data wrangling functionality of pandas dataframes can be applied to our spatial vector data as well.\n",
        "\n",
        "### A few differences include:\n",
        "The GeoDataFrame contains a geometry column which stores spatial information. The geometry column in your GeoDataFrame stores the boundary information (the lines that make up each shape in your data). This allows you to plot points, lines or polygons.\n",
        "\n",
        "The GeoDataFrame stores spatial attributes such as coordinate reference systems and spatial extents.\n",
        "\n",
        "Similar to Pandas, you can plot the data using .plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf3hUYvnAWR-"
      },
      "outputs": [],
      "source": [
        "# Plot the data\n",
        "f, ax1 = plt.subplots(figsize=(12, 6))\n",
        "coastlines.plot(ax=ax1)\n",
        "\n",
        "# Add a title to your plot\n",
        "ax1.set(title=\"Global Coastline Boundaries\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiF7ytzZAWR_"
      },
      "source": [
        "### Check the Spatial Vector Data Type\n",
        "You can look at the data to figure out what type of data are stored in the shapefile (points, line or polygons). However, you can also get that information by calling .geom_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw9ujP22AWR_"
      },
      "outputs": [],
      "source": [
        "# Is the geometry type point, line or polygon?\n",
        "coastlines.geom_type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z3XseEqAWSA"
      },
      "source": [
        "Also similar to Pandas, you can view descriptive information about the GeoDataFrame using .info(). This includes the number of columns, rows and the header name and type of each column.¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMNnZaOUAWSB"
      },
      "outputs": [],
      "source": [
        "coastlines.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6t7ulmJAWSC"
      },
      "source": [
        "### Open point data\n",
        "Next, you will open up another shapefile using Geopandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMCHAAnXAWSC"
      },
      "outputs": [],
      "source": [
        "# Open a second layer\n",
        "et.data.get_data(\n",
        "    url='https://naturalearth.s3.amazonaws.com/50m_cultural/ne_50m_populated_places_simple.zip')\n",
        "\n",
        "# Create a path to the populated places shapefile\n",
        "populated_places_path = os.path.join(\"data\", \"earthpy-downloads\",\n",
        "                                     \"ne_50m_populated_places_simple\",\n",
        "                                     \"ne_50m_populated_places_simple.shp\")\n",
        "\n",
        "#read in a new geopandas data frame called \"cities\"\n",
        "cities = gpd.read_file(populated_places_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuHhWHjiAWSD"
      },
      "source": [
        "### TASK 1: Is cities a point, line, or polygon file? Type a command to find out and interpret the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6BEiY2ZAWSD"
      },
      "outputs": [],
      "source": [
        "# Task 1 answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWs792nPAWSD"
      },
      "source": [
        "The attributes for a shapefile imported into a GeoDataFrame can be viewed in the GeoDataFrame itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zU47qq6jAWSD"
      },
      "outputs": [],
      "source": [
        "# View first 5 rows of GeoDataFrame\n",
        "cities.head()\n",
        "\n",
        "\n",
        "# Experiment! How would you view the first 10 rows of a GeoDataFrame?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4zDORD5AWSE"
      },
      "source": [
        "Just like with Pandas DataFrames, standard arguments can be used to calculate summary statistics on your GeoPandas object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZlzvjoVAWSE"
      },
      "outputs": [],
      "source": [
        "#Let's see what our most populous city is\n",
        "#print out the column with maximum population\n",
        "print(cities.pop_max)\n",
        "print(cities.iloc[cities.pop_max.idxmax()])\n",
        "\n",
        "print(\"The world's largest city  by population is \" + cities.name.iloc[cities.pop_max.idxmax()] + \n",
        "      \" with \" + str(cities.pop_max.max()) + \n",
        "      \" people!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Geopandas = geographic pandas data frames\n",
        "The handy thing about working with vector data in geopandas is that you have all of the functionality of pandas at your fingertips. This means that it is easy to do things like:\n",
        "1. Subset your shapefile based on certain crtiera\n",
        "2. Merge your shapefile with other datasets\n",
        "3. Process fields (columns) in your shapefile using python commands.\n",
        "\n",
        "For example, let's say we are conducting a study on megacities. We can easily identify our megacities using the pop_max field, and use that to create a new geopandas DataFrame of only megacities (cities with a population greater than 10 million."
      ],
      "metadata": {
        "id": "bD4altg7ZcY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "megacities = cities[cities.pop_max >= 10000000]\n",
        "print(megacities)\n",
        "type(megacities)"
      ],
      "metadata": {
        "id": "RgmN45l7X9x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsIQrDG7AWSF"
      },
      "source": [
        "### Creating Maps Using Multiple Shapefiles\n",
        "You can create maps using multiple shapefiles with Geopandas in a similar way that you may do so using a graphical user interface (GUI) tool like ArcGIS or QGIS (open source alternative to ArcGIS). To do this you will need to open a second spatial file. Below you will use the Natural Earth populated places shapefile to add additional layers to your map.\n",
        "\n",
        "To plot two datasets together, you will first create a Matplotlib figure object. Notice in the example below that you define the figure ax1 in the first line. You then tell GeoPandas to plot the data on that particular figure using the parameter ax=\n",
        "\n",
        "The code looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbKWWShmAWSF"
      },
      "outputs": [],
      "source": [
        "#Experiment! \n",
        "#What happens if you change FigSize numbers? \n",
        "#What happens if you \"comment out\" (put a # in front of) plt.show()? \n",
        "#What happens if you change the color?\n",
        "\n",
        "f, ax1 = plt.subplots(figsize=(10, 6))\n",
        "coastlines.plot(ax=ax1,\n",
        "               color = \"black\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMXUgNKcAWSG"
      },
      "source": [
        "To add another layer to your map, you can add a second .plot() call and specify the ax= to be ax1 again. This tells Python to layer the two datasets in the same figure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lE915VxAWSG"
      },
      "outputs": [],
      "source": [
        "# Create a map or plot with two data layers\n",
        "\n",
        "#Experiment! What happens if you remove ax=ax1 from coastlines.plot() and cities.plot()?\n",
        "\n",
        "f, ax1 = plt.subplots(figsize=(10, 6))\n",
        "coastlines.plot(ax=ax1,\n",
        "                color=\"black\")\n",
        "cities.plot(ax=ax1)\n",
        "\n",
        "# Add a title\n",
        "ax1.set(title=\"Map of Cities and Global Lines\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJNsynPYAWSG"
      },
      "source": [
        "Learn more about custom plotting in Python:\n",
        "\n",
        "https://www.earthdatascience.org/courses/scientists-guide-to-plotting-data-in-python/plot-spatial-data/customize-vector-plots/python-customize-map-legends-geopandas/\n",
        "\n",
        "https://geopandas.org/gallery/plotting_with_geoplot.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZqRAA7CAWSH"
      },
      "source": [
        "### TASK 2\n",
        "Using the resources above, create a map that contains:\n",
        "1. Coastal lines (black)\n",
        "2. Cities, excluding megacities (points), with a color scale for pop_max\n",
        "3. Megacities (large black points)\n",
        "3. A legend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKAakadnAWSH"
      },
      "outputs": [],
      "source": [
        "# Format and plot your results from task 2\n",
        "# hint:\n",
        "?cities.plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvqqbRFfAWSI"
      },
      "source": [
        "### Geoprocessing Vector Data Geoprocessing in Python: Clip Data\n",
        "Sometimes you have spatial data for a larger area than you need to process. For example you may be working on a project for your state or country. But perhaps you have data for the entire globe.\n",
        "\n",
        "You can clip the data spatially to another boundary to make it smaller. Once the data are clipped, your processing operations will be faster. It will also make creating maps of your study area easier and cleaner.\n",
        "\n",
        "In this workflow, you'll subset your cities data to only look at counties in the United States. First, we'll import a dataset of global political boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBQLJxdoAWSJ"
      },
      "outputs": [],
      "source": [
        "country_data_url = \"https://naturalearth.s3.amazonaws.com/50m_cultural/ne_50m_admin_0_countries.zip\"\n",
        "et.data.get_data(url=country_data_url)\n",
        "\n",
        "# Create a path to the countries shapefile\n",
        "countries_path = os.path.join(\"data\", \"earthpy-downloads\",\n",
        "                              \"ne_50m_admin_0_countries\", \n",
        "                              \"ne_50m_admin_0_countries.shp\")\n",
        "\n",
        "# Read in the countries shapefile as GeoPandas dataframe\n",
        "countries = gpd.read_file(countries_path)\n",
        "\n",
        "# View attribute table:\n",
        "countries.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlWM-PnWAWSJ"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(10, 6))\n",
        "countries.plot(ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46TuC7lPAWSK"
      },
      "source": [
        "### Next, we'll use built-in pandas funcitonality to subset the shapefile to just the US boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMmfKI3TAWSK"
      },
      "outputs": [],
      "source": [
        "# Subset the countries data to just a single country\n",
        "united_states_boundary = countries.loc[countries['SOVEREIGNT']\n",
        "                                       == 'United States of America']\n",
        "\n",
        "# Notice in the plot below, that only the boundary for the USA is in the new variable\n",
        "f, ax = plt.subplots(figsize=(10, 6))\n",
        "united_states_boundary.plot(ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz7Xo5MWAWSL"
      },
      "source": [
        "### Now, we'll subset the cities layer to include only records which overlap in space with the united_states_boundary layer (aka are in the United States)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiVaflFIAWSL"
      },
      "outputs": [],
      "source": [
        "# Clip the cities data to the USA boundary\n",
        "# Note -- this operation may take some time to run - be patient\n",
        "cities_in_usa = gpd.clip(cities, united_states_boundary)\n",
        "\n",
        "# Plot your final clipped data\n",
        "f, ax = plt.subplots()\n",
        "cities_in_usa.plot(ax=ax)\n",
        "ax.set(title=\"Cities clipped to the USA Boundary\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 3: How many cities are in the United States?"
      ],
      "metadata": {
        "id": "9s4e3Ll4cSPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 code here"
      ],
      "metadata": {
        "id": "5cfuchoecP6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC497LtLAWSM"
      },
      "source": [
        "## TASK 4: What is the largest city in the United States? What is the population of that city?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXb-CVCjAWSM"
      },
      "outputs": [],
      "source": [
        "#Task 4 code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging vector data in geopandas\n",
        "One of our primary goals using geographic information systems (GIS) software in Environmental Data Science is to collocate information from different sources into a single analaysis-ready dataset. When we're working with vector data, this often means adding new columns to a geopandas dataframe containing values from another dataset. \n",
        "\n",
        "There are two ways to combine datasets in geopandas – attribute joins and spatial joins. [From Geopandas.org](https://geopandas.org/en/v0.8.2/mergingdata.html)\n",
        "\n",
        "*   **Attribute joins:** a GeoSeries or GeoDataFrame is combined with a regular pandas Series or DataFrame based on a common variable. This is analogous to normal merging or joining in pandas.\n",
        "\n",
        "*   **Spatial joins:** a Spatial Join, observations from two GeoSeries or GeoDataFrames are combined based on their spatial relationship to one another.Indented block\n",
        "\n",
        "First, we'll bring in another dataset of countries:\n"
      ],
      "metadata": {
        "id": "CUH9oYodc47X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read in country shapefile from the geopandas datasets collection\n",
        "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
        "print(world)"
      ],
      "metadata": {
        "id": "hqofNW42cmkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say we want to add the \"continent\" field (column) in the world geodataframe to the cities dataframe. Using an attribute join, we first find two columns that contain the same values:"
      ],
      "metadata": {
        "id": "8Lbpg8J2gZTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sorted(world.iso_a3.unique()))\n",
        "print(sorted(cities.sov_a3.unique()))\n",
        "print(\"There are \" + str(len(world.iso_a3.unique()))+ \" unique country codes in the world dataset\")\n",
        "print(\"There are \" + str(len(cities.sov_a3.unique()))+ \" unique country codes in the city dataset\") "
      ],
      "metadata": {
        "id": "vNJfea09gVcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do not have continent data for all of the countries represented in the city database, but we have some!"
      ],
      "metadata": {
        "id": "BSyYd4DIkpyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we'll make a new pandas dataframe containing only the  iso_a3, and continent information:\n",
        "continents = world[['iso_a3', 'continent']]"
      ],
      "metadata": {
        "id": "SEtpsbkvkncQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then, we'll merge this data to the cities dataset:\n",
        "cities = cities.merge(continents, left_on='sov_a3', right_on='iso_a3')\n",
        "\n",
        "# Is cities still a geopandas dataframe?\n",
        "type(cities)\n",
        "\n",
        "#On what continents do we have cities?\n",
        "cities.continent.unique()"
      ],
      "metadata": {
        "id": "yVRmCKVQkQFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A spatial join involves fusing two datasets based on the euclidian distances calculated between feature locations.\n",
        "\n",
        "\n",
        "Sjoin Arguments\n",
        "\n",
        "`sjoin()` has two core arguments: how and op.\n",
        "\n",
        "**op**\n",
        "\n",
        "The op argument specifies how geopandas decides whether or not to join the attributes of one object to another. There are three different join options as follows:\n",
        "\n",
        "    intersects: The attributes will be joined if the boundary and interior of the object intersect in any way with the boundary and/or interior of the other object.\n",
        "\n",
        "    within: The attributes will be joined if the object’s boundary and interior intersect only with the interior of the other object (not its boundary or exterior).\n",
        "\n",
        "    contains: The attributes will be joined if the object’s interior contains the boundary and interior of the other object and their boundaries do not touch at all.\n",
        "\n",
        "You can read more about each join type in the Shapely documentation.\n",
        "\n",
        "**how**\n",
        "\n",
        "The how argument specifies the type of join that will occur and which geometry is retained in the resultant geodataframe. It accepts the following options:\n",
        "\n",
        "    left: use the index from the first (or left_df) geodataframe that you provide to sjoin; retain only the left_df geometry column\n",
        "\n",
        "    right: use index from second (or right_df); retain only the right_df geometry column\n",
        "\n",
        "    inner: use intersection of index values from both geodataframes; retain only the left_df geometry column\n",
        "\n",
        "Note more complicated spatial relationships can be studied by combining geometric operations with spatial join. To find all polygons within a given distance of a point, for example, one can first use the buffer method to expand each point into a circle of appropriate radius, then intersect those buffered circles with the polygons in question.\n"
      ],
      "metadata": {
        "id": "KFSeBixZmB9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a countries shapefile with only one attribute (column), called 'name':\n",
        "countries = world[['geometry', 'name']]\n",
        "countries.head()\n"
      ],
      "metadata": {
        "id": "6ptYDRH6mBnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall all the attributes of cities:\n",
        "cities.head()"
      ],
      "metadata": {
        "id": "Q87DNaF4nCHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check and see if the merge worked\n",
        "cities_with_country = gpd.sjoin(cities, countries, how=\"inner\", op='intersects')\n",
        "cities_with_country.head()"
      ],
      "metadata": {
        "id": "bszSgKS-nGOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 5: \n",
        "In this markdown cell in your own words, explain how the spatial join worked. How does this differ from the attribute join?"
      ],
      "metadata": {
        "id": "IRdwL71Xs6MR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nested features\n",
        "Often, vector spatial features will be nested within each other. For example, in the world dataset, there are countries which are nested within continents.\n",
        "\n",
        "If you wanted to extract just the continent outlines, you would need to erase the country boundaries within each continent. We can use the **dissolve** function to do this.\n",
        "\n",
        "In a non-spatial setting, when all we need are summary statistics of the data, we aggregate our data using the `groupby` function. But for spatial data, we sometimes alsoIn a non-spatial setting, when all we need are summary statistics of the data, we aggregate our data using the groupby function. But for spatial data, we sometimes also need to aggregate geometric features. In the geopandas library, we can aggregate geometric features using the dissolve function.\n",
        "\n",
        "dissolve can be thought of as doing three things: (a) it dissolves all the geometries within a given group together into a single geometric feature (using the unary_union method), and (b) it aggregates all the rows of data in a group using groupby.aggregate(), and (c) it combines those two results.\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        " need to aggregate geometric features. In the geopandas library, we can aggregate geometric features using the `dissolve` function.\n",
        "\n",
        "`dissolve` can be thought of as doing three things: (a) it dissolves all the geometries within a given group together into a single geometric feature (using the unary_union method), and (b) it aggregates all the rows of data in a group using `groupby.aggregate()`, and (c) it combines those two results."
      ],
      "metadata": {
        "id": "FRn005lstNno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "world = world[['continent', 'geometry']]\n",
        "\n",
        "continents = world.dissolve(by='continent')\n",
        "\n",
        "continents.plot();\n",
        "\n",
        "continents.head()"
      ],
      "metadata": {
        "id": "oZZ6er1Et_XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we are interested in aggregate populations, however, we can pass different functions to the dissolve method to aggregate populations using the `aggfunc = ` argument:"
      ],
      "metadata": {
        "id": "pna_Qn2aurYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-read in the world dataset\n",
        "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
        "\n",
        "# Select only the attributes we're interested in:\n",
        "world = world[['continent', 'geometry', 'pop_est']]\n",
        "\n",
        "# Dissolve into country aggregates, collecting the sum of other attributes\n",
        "continents = world.dissolve(by='continent', aggfunc='sum')\n",
        "\n",
        "continents.plot(column = 'pop_est', scheme='quantiles', cmap='YlOrRd');\n",
        "continents.head()"
      ],
      "metadata": {
        "id": "IbG_sgdiuyX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 6-8: putting it together\n",
        "Use your skills with `merge`, `dissolve`, and pandas math to do the following:\n",
        "6. What is the total urban population on each continent? Show your results as a chloropleth map and a table.\n",
        "7. What fraction of the total population lives in cities in each continent? Show your results as a chloropleth map and a table.\n",
        "8. What is the most urban continent on earth (largest share of the population lives in cities)?"
      ],
      "metadata": {
        "id": "ILJ15rlfvjRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Task 6 code here:"
      ],
      "metadata": {
        "id": "Ij3UEmxqvNwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6 code here:"
      ],
      "metadata": {
        "id": "9Xzpz9CCwc2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8 code here:"
      ],
      "metadata": {
        "id": "G-FlMvOnwgdF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}